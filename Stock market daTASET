# importing necessary libaries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from tensorflow.keras.models import Sequential  
from tensorflow.keras.layers import Dense       
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dropout
from keras.layers import SimpleRNN
from tensorflow.keras.optimizers import Adam
import math
from sklearn.metrics import mean_squared_error
# importing dataset
# data = pd.read_csv('/kaggle/input/price-volume-data-for-all-us-stocks-etfs/Stocks/abb.us.txt', parse_dates=['Date'], sep=',', index_col='Date')
data = pd.read_csv('/kaggle/input/price-volume-data-for-all-us-stocks-etfs/Stocks/aimc.us.txt', parse_dates=['Date'], sep=',', index_col='Date')
# shows first 10 rows
data.head()
Open	High	Low	Close	Volume	OpenInt
Date						
2006-12-14	12.330	12.330	12.330	12.330	0	0
2006-12-15	12.330	13.050	12.200	13.050	5622785	0
2006-12-18	12.695	12.988	12.358	12.777	466489	0
2006-12-19	12.513	12.602	12.330	12.593	316425	0
2006-12-20	12.330	12.602	12.330	12.555	170058	0
# shows last 10 rows
data.tail()
Open	High	Low	Close	Volume	OpenInt
Date						
2017-11-06	47.80	47.90	46.70	46.85	137985	0
2017-11-07	46.75	46.80	46.00	46.00	83142	0
2017-11-08	45.90	46.00	45.25	46.00	124742	0
2017-11-09	45.65	46.25	45.00	46.15	105979	0
2017-11-10	46.10	46.50	45.90	46.45	91648	0
# details about the columns
data.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 2746 entries, 2006-12-14 to 2017-11-10
Data columns (total 6 columns):
 #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   Open     2746 non-null   float64
 1   High     2746 non-null   float64
 2   Low      2746 non-null   float64
 3   Close    2746 non-null   float64
 4   Volume   2746 non-null   int64  
 5   OpenInt  2746 non-null   int64  
dtypes: float64(4), int64(2)
memory usage: 150.2 KB
#Checking number of null values
data.isnull().sum() 
Open       0
High       0
Low        0
Close      0
Volume     0
OpenInt    0
dtype: int64
#Plotting the changes in closing price over time

#Size of the plot
plt.figure(figsize = (12,8)) 
#Giving label on X axis
plt.xlabel('Year',fontsize=12) 
#Giving label on Y axis
plt.ylabel('Closing price',fontsize=12) 
#Title of the plot
plt.title('Altra Indrustrial Motion Corporation Stock Price Changes Over Time',fontsize=15) 
plt.plot(data.index,data['Close']) #Plotting data.index as x axis and close column as y axis
[<matplotlib.lines.Line2D at 0x7fd280549f10>]

# splitting the dataset into train and test
training_size=int(len(data)*0.8)
test_size=int(len(data)-training_size)
train_data=data[:training_size]
test_data=data[training_size:]
# Scaling down the dataset between 0,1
scaler1=MinMaxScaler(feature_range=(0,1)) #Scaling down the values between 0 to 1
scaler2=MinMaxScaler(feature_range=(0,1)) #Scaling down the values between 0 to 1
# window size
timestep=100
# number of target features
no_of_features=1
# function to transform the data set 
def prepare_dataset(data1,timestep,scaler):
  s=scaler
  data=s.fit_transform(np.array(data1['Close']).reshape(-1,1)) 
  X,Y=[],[] 
  for i in range(len(data)-timestep):
    a=data[i:i+timestep]
    X.append(a)
    Y.append(data[i+timestep])
  return np.array(X),np.array(Y)
#Preparing the dataset
X_train,Y_train=prepare_dataset(train_data,timestep,scaler1)
X_test,Y_test=prepare_dataset(test_data,timestep,scaler2)
#Creating the RNN model 

# Initialising the  Model
model2=Sequential()
# SimpleRNN(hidden neurons, Carries output to next hidden layer )
model2.add(SimpleRNN(7,input_shape=(timestep,no_of_features),return_sequences=False))
# Dropout layer to avoid overfitting
# model2.add(Dropout(0.3))
# dense layer for output
model2.add(Dense(1))
model2.compile(loss='mean_squared_error',optimizer='adam')
model2.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn (SimpleRNN)      (None, 7)                 63        
                                                                 
 dense (Dense)               (None, 1)                 8         
                                                                 
=================================================================
Total params: 71
Trainable params: 71
Non-trainable params: 0
_________________________________________________________________
# fitting the model on training dataset
model3=model2.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=100,batch_size=64,verbose=1) 
Epoch 1/100
33/33 [==============================] - 2s 34ms/step - loss: 0.3493 - val_loss: 0.2560
Epoch 2/100
33/33 [==============================] - 1s 23ms/step - loss: 0.1360 - val_loss: 0.1190
Epoch 3/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0653 - val_loss: 0.0514
Epoch 4/100
33/33 [==============================] - 1s 27ms/step - loss: 0.0294 - val_loss: 0.0257
Epoch 5/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0158 - val_loss: 0.0168
Epoch 6/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0102 - val_loss: 0.0123
Epoch 7/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0076 - val_loss: 0.0100
Epoch 8/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0061 - val_loss: 0.0086
Epoch 9/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0051 - val_loss: 0.0075
Epoch 10/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0043 - val_loss: 0.0066
Epoch 11/100
33/33 [==============================] - 1s 27ms/step - loss: 0.0038 - val_loss: 0.0060
Epoch 12/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0034 - val_loss: 0.0056
Epoch 13/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0031 - val_loss: 0.0053
Epoch 14/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0028 - val_loss: 0.0049
Epoch 15/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0026 - val_loss: 0.0045
Epoch 16/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0024 - val_loss: 0.0043
Epoch 17/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0023 - val_loss: 0.0042
Epoch 18/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.0039
Epoch 19/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0021 - val_loss: 0.0039
Epoch 20/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0020 - val_loss: 0.0038
Epoch 21/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0019 - val_loss: 0.0036
Epoch 22/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0018 - val_loss: 0.0035
Epoch 23/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0018 - val_loss: 0.0035
Epoch 24/100
33/33 [==============================] - 1s 25ms/step - loss: 0.0017 - val_loss: 0.0034
Epoch 25/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0016 - val_loss: 0.0033
Epoch 26/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0016 - val_loss: 0.0032
Epoch 27/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0016 - val_loss: 0.0031
Epoch 28/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0015 - val_loss: 0.0031
Epoch 29/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0015 - val_loss: 0.0031
Epoch 30/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.0031
Epoch 31/100
33/33 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.0030
Epoch 32/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0014 - val_loss: 0.0029
Epoch 33/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0013 - val_loss: 0.0029
Epoch 34/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0013 - val_loss: 0.0029
Epoch 35/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0013 - val_loss: 0.0028
Epoch 36/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0013 - val_loss: 0.0028
Epoch 37/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0012 - val_loss: 0.0027
Epoch 38/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0012 - val_loss: 0.0027
Epoch 39/100
33/33 [==============================] - 1s 25ms/step - loss: 0.0012 - val_loss: 0.0026
Epoch 40/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0012 - val_loss: 0.0026
Epoch 41/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0011 - val_loss: 0.0026
Epoch 42/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0011 - val_loss: 0.0025
Epoch 43/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0011 - val_loss: 0.0025
Epoch 44/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0011 - val_loss: 0.0025
Epoch 45/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0011 - val_loss: 0.0024
Epoch 46/100
33/33 [==============================] - 1s 24ms/step - loss: 0.0010 - val_loss: 0.0024
Epoch 47/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0010 - val_loss: 0.0024
Epoch 48/100
33/33 [==============================] - 1s 26ms/step - loss: 0.0010 - val_loss: 0.0024
Epoch 49/100
33/33 [==============================] - 1s 26ms/step - loss: 9.9991e-04 - val_loss: 0.0023
Epoch 50/100
33/33 [==============================] - 1s 27ms/step - loss: 9.8348e-04 - val_loss: 0.0023
Epoch 51/100
33/33 [==============================] - 1s 24ms/step - loss: 9.6990e-04 - val_loss: 0.0023
Epoch 52/100
33/33 [==============================] - 1s 24ms/step - loss: 9.5094e-04 - val_loss: 0.0022
Epoch 53/100
33/33 [==============================] - 1s 24ms/step - loss: 9.5184e-04 - val_loss: 0.0023
Epoch 54/100
33/33 [==============================] - 1s 24ms/step - loss: 9.2928e-04 - val_loss: 0.0022
Epoch 55/100
33/33 [==============================] - 1s 23ms/step - loss: 9.1135e-04 - val_loss: 0.0021
Epoch 56/100
33/33 [==============================] - 1s 23ms/step - loss: 8.9257e-04 - val_loss: 0.0021
Epoch 57/100
33/33 [==============================] - 1s 23ms/step - loss: 8.8676e-04 - val_loss: 0.0021
Epoch 58/100
33/33 [==============================] - 1s 26ms/step - loss: 8.7389e-04 - val_loss: 0.0021
Epoch 59/100
33/33 [==============================] - 1s 24ms/step - loss: 8.5556e-04 - val_loss: 0.0020
Epoch 60/100
33/33 [==============================] - 1s 24ms/step - loss: 8.4004e-04 - val_loss: 0.0020
Epoch 61/100
33/33 [==============================] - 1s 24ms/step - loss: 8.3275e-04 - val_loss: 0.0019
Epoch 62/100
33/33 [==============================] - 1s 24ms/step - loss: 8.2530e-04 - val_loss: 0.0020
Epoch 63/100
33/33 [==============================] - 1s 23ms/step - loss: 8.2263e-04 - val_loss: 0.0019
Epoch 64/100
33/33 [==============================] - 1s 24ms/step - loss: 7.9311e-04 - val_loss: 0.0019
Epoch 65/100
33/33 [==============================] - 1s 24ms/step - loss: 7.8503e-04 - val_loss: 0.0019
Epoch 66/100
33/33 [==============================] - 1s 23ms/step - loss: 7.7707e-04 - val_loss: 0.0018
Epoch 67/100
33/33 [==============================] - 1s 26ms/step - loss: 7.6166e-04 - val_loss: 0.0018
Epoch 68/100
33/33 [==============================] - 1s 26ms/step - loss: 7.6238e-04 - val_loss: 0.0019
Epoch 69/100
33/33 [==============================] - 1s 27ms/step - loss: 7.5006e-04 - val_loss: 0.0018
Epoch 70/100
33/33 [==============================] - 1s 27ms/step - loss: 7.3391e-04 - val_loss: 0.0017
Epoch 71/100
33/33 [==============================] - 1s 23ms/step - loss: 7.1820e-04 - val_loss: 0.0017
Epoch 72/100
33/33 [==============================] - 1s 24ms/step - loss: 7.1326e-04 - val_loss: 0.0017
Epoch 73/100
33/33 [==============================] - 1s 24ms/step - loss: 7.0444e-04 - val_loss: 0.0017
Epoch 74/100
33/33 [==============================] - 1s 24ms/step - loss: 6.8681e-04 - val_loss: 0.0016
Epoch 75/100
33/33 [==============================] - 1s 24ms/step - loss: 6.8216e-04 - val_loss: 0.0016
Epoch 76/100
33/33 [==============================] - 1s 24ms/step - loss: 6.8591e-04 - val_loss: 0.0016
Epoch 77/100
33/33 [==============================] - 1s 24ms/step - loss: 6.6324e-04 - val_loss: 0.0016
Epoch 78/100
33/33 [==============================] - 1s 27ms/step - loss: 6.5799e-04 - val_loss: 0.0016
Epoch 79/100
33/33 [==============================] - 1s 26ms/step - loss: 6.4237e-04 - val_loss: 0.0016
Epoch 80/100
33/33 [==============================] - 1s 23ms/step - loss: 6.3248e-04 - val_loss: 0.0015
Epoch 81/100
33/33 [==============================] - 1s 26ms/step - loss: 6.3210e-04 - val_loss: 0.0015
Epoch 82/100
33/33 [==============================] - 1s 30ms/step - loss: 6.1802e-04 - val_loss: 0.0015
Epoch 83/100
33/33 [==============================] - 1s 25ms/step - loss: 6.1414e-04 - val_loss: 0.0015
Epoch 84/100
33/33 [==============================] - 1s 24ms/step - loss: 6.0199e-04 - val_loss: 0.0014
Epoch 85/100
33/33 [==============================] - 1s 24ms/step - loss: 6.0152e-04 - val_loss: 0.0014
Epoch 86/100
33/33 [==============================] - 1s 23ms/step - loss: 5.9447e-04 - val_loss: 0.0014
Epoch 87/100
33/33 [==============================] - 1s 26ms/step - loss: 5.9744e-04 - val_loss: 0.0014
Epoch 88/100
33/33 [==============================] - 1s 25ms/step - loss: 5.6988e-04 - val_loss: 0.0014
Epoch 89/100
33/33 [==============================] - 1s 24ms/step - loss: 5.6318e-04 - val_loss: 0.0013
Epoch 90/100
33/33 [==============================] - 1s 25ms/step - loss: 5.6224e-04 - val_loss: 0.0014
Epoch 91/100
33/33 [==============================] - 1s 24ms/step - loss: 5.5331e-04 - val_loss: 0.0013
Epoch 92/100
33/33 [==============================] - 1s 24ms/step - loss: 5.3983e-04 - val_loss: 0.0013
Epoch 93/100
33/33 [==============================] - 1s 24ms/step - loss: 5.3174e-04 - val_loss: 0.0013
Epoch 94/100
33/33 [==============================] - 1s 23ms/step - loss: 5.2826e-04 - val_loss: 0.0013
Epoch 95/100
33/33 [==============================] - 1s 24ms/step - loss: 5.2060e-04 - val_loss: 0.0012
Epoch 96/100
33/33 [==============================] - 1s 24ms/step - loss: 5.1431e-04 - val_loss: 0.0013
Epoch 97/100
33/33 [==============================] - 1s 25ms/step - loss: 5.2785e-04 - val_loss: 0.0012
Epoch 98/100
33/33 [==============================] - 1s 24ms/step - loss: 5.1473e-04 - val_loss: 0.0012
Epoch 99/100
33/33 [==============================] - 1s 25ms/step - loss: 4.9828e-04 - val_loss: 0.0012
Epoch 100/100
33/33 [==============================] - 1s 25ms/step - loss: 4.8977e-04 - val_loss: 0.0012
#Plotting the loss
plt.plot(model3.history['loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

train_Predict=model2.predict(X_train)              
test_Predict=model2.predict(X_test)
train_Predict=scaler1.inverse_transform(train_Predict)       #getting back original form for plotting
test_Predict=scaler2.inverse_transform(test_Predict)
train_data1=scaler1.inverse_transform(Y_train)
test_data1=scaler2.inverse_transform(Y_test)
66/66 [==============================] - 1s 8ms/step
15/15 [==============================] - 0s 8ms/step
# Getting rmse of train data
print("RNN Model Train RMSE %0.2f"%(math.sqrt(mean_squared_error(train_data1,train_Predict))))
RNN Model Train RMSE 0.72
# Getting rmse of test data
print("RNN Model Test RMSE %0.2f"%(math.sqrt(mean_squared_error(test_data1,test_Predict))))
RNN Model Test RMSE 1.02
df_1 = pd.DataFrame(test_data1, columns = ['Original_Test_Data'])
df_2 = pd.DataFrame(test_Predict, columns = ['Predicted_Test_Data'])
df_1.reset_index()
df_2.reset_index()
result = pd.merge(df_1, df_2, left_index=True, right_index=True)
result.head(10)
Original_Test_Data	Predicted_Test_Data
0	21.170	22.140717
1	21.354	21.473045
2	21.500	21.374149
3	21.007	21.418192
4	21.346	21.767530
5	21.063	21.691919
6	20.907	21.415323
7	21.044	21.058182
8	21.227	21.250139
9	21.277	21.415857
#plotting the models performance against the train data
plt.figure(figsize = (18,6))

plt.subplot(1,2,1)
plt.title('Prediction in terms of Training Data',fontsize=10)
plt.plot(train_data1,label='Actual')
plt.plot(train_Predict,label='Predicted')
leg=plt.legend()

plt.subplot(1,2,2)
plt.title('Prediction in terms of Test data',fontsize=10)
plt.plot(test_data1,label='Actual')
plt.plot(test_Predict,label='Predicted')
leg=plt.legend()
plt.show()

#Creating the LSTM model 

# Initialising the  Model
model=Sequential()   
# LSTM(hidden neurons, Carries output to next hidden layer )
model.add(LSTM(7,return_sequences=False,input_shape=(timestep,no_of_features)))    
# Dropout layer to avoid overfitting
# model.add(Dropout(0.3))
# dense layer for output
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
# model summary
model.summary()  
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 7)                 252       
                                                                 
 dense_1 (Dense)             (None, 1)                 8         
                                                                 
=================================================================
Total params: 260
Trainable params: 260
Non-trainable params: 0
_________________________________________________________________
# fitting the model on training dataset
model1=model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=100,batch_size=64,verbose=1) 
Epoch 1/100
33/33 [==============================] - 5s 64ms/step - loss: 0.3533 - val_loss: 0.2942
Epoch 2/100
33/33 [==============================] - 2s 47ms/step - loss: 0.1777 - val_loss: 0.1356
Epoch 3/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0656 - val_loss: 0.0484
Epoch 4/100
33/33 [==============================] - 2s 48ms/step - loss: 0.0321 - val_loss: 0.0322
Epoch 5/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0208 - val_loss: 0.0186
Epoch 6/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0106 - val_loss: 0.0078
Epoch 7/100
33/33 [==============================] - 2s 50ms/step - loss: 0.0036 - val_loss: 0.0028
Epoch 8/100
33/33 [==============================] - 2s 52ms/step - loss: 0.0016 - val_loss: 0.0023
Epoch 9/100
33/33 [==============================] - 2s 49ms/step - loss: 0.0014 - val_loss: 0.0022
Epoch 10/100
33/33 [==============================] - 2s 47ms/step - loss: 0.0013 - val_loss: 0.0020
Epoch 11/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0013 - val_loss: 0.0020
Epoch 12/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0012 - val_loss: 0.0020
Epoch 13/100
33/33 [==============================] - 2s 48ms/step - loss: 0.0012 - val_loss: 0.0019
Epoch 14/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0012 - val_loss: 0.0019
Epoch 15/100
33/33 [==============================] - 1s 45ms/step - loss: 0.0011 - val_loss: 0.0018
Epoch 16/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0011 - val_loss: 0.0019
Epoch 17/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0011 - val_loss: 0.0018
Epoch 18/100
33/33 [==============================] - 2s 47ms/step - loss: 0.0010 - val_loss: 0.0018
Epoch 19/100
33/33 [==============================] - 2s 46ms/step - loss: 0.0010 - val_loss: 0.0019
Epoch 20/100
33/33 [==============================] - 2s 45ms/step - loss: 0.0010 - val_loss: 0.0018
Epoch 21/100
33/33 [==============================] - 2s 48ms/step - loss: 9.7251e-04 - val_loss: 0.0017
Epoch 22/100
33/33 [==============================] - 1s 45ms/step - loss: 9.5252e-04 - val_loss: 0.0017
Epoch 23/100
33/33 [==============================] - 2s 47ms/step - loss: 9.2974e-04 - val_loss: 0.0018
Epoch 24/100
33/33 [==============================] - 2s 49ms/step - loss: 9.1432e-04 - val_loss: 0.0016
Epoch 25/100
33/33 [==============================] - 2s 48ms/step - loss: 8.8941e-04 - val_loss: 0.0017
Epoch 26/100
33/33 [==============================] - 2s 48ms/step - loss: 8.7476e-04 - val_loss: 0.0016
Epoch 27/100
33/33 [==============================] - 2s 48ms/step - loss: 8.5702e-04 - val_loss: 0.0016
Epoch 28/100
33/33 [==============================] - 2s 52ms/step - loss: 8.3746e-04 - val_loss: 0.0016
Epoch 29/100
33/33 [==============================] - 2s 46ms/step - loss: 8.2052e-04 - val_loss: 0.0016
Epoch 30/100
33/33 [==============================] - 2s 46ms/step - loss: 8.1489e-04 - val_loss: 0.0015
Epoch 31/100
33/33 [==============================] - 2s 47ms/step - loss: 8.0363e-04 - val_loss: 0.0014
Epoch 32/100
33/33 [==============================] - 2s 47ms/step - loss: 7.8020e-04 - val_loss: 0.0015
Epoch 33/100
33/33 [==============================] - 2s 46ms/step - loss: 7.5752e-04 - val_loss: 0.0014
Epoch 34/100
33/33 [==============================] - 2s 47ms/step - loss: 7.4526e-04 - val_loss: 0.0015
Epoch 35/100
33/33 [==============================] - 2s 48ms/step - loss: 7.3217e-04 - val_loss: 0.0014
Epoch 36/100
33/33 [==============================] - 2s 48ms/step - loss: 7.2331e-04 - val_loss: 0.0014
Epoch 37/100
33/33 [==============================] - 2s 48ms/step - loss: 7.0816e-04 - val_loss: 0.0013
Epoch 38/100
33/33 [==============================] - 2s 48ms/step - loss: 6.9996e-04 - val_loss: 0.0014
Epoch 39/100
33/33 [==============================] - 2s 46ms/step - loss: 6.8177e-04 - val_loss: 0.0014
Epoch 40/100
33/33 [==============================] - 2s 47ms/step - loss: 6.7329e-04 - val_loss: 0.0013
Epoch 41/100
33/33 [==============================] - 2s 49ms/step - loss: 6.6151e-04 - val_loss: 0.0013
Epoch 42/100
33/33 [==============================] - 2s 50ms/step - loss: 6.4928e-04 - val_loss: 0.0013
Epoch 43/100
33/33 [==============================] - 2s 49ms/step - loss: 6.4237e-04 - val_loss: 0.0013
Epoch 44/100
33/33 [==============================] - 2s 46ms/step - loss: 6.2690e-04 - val_loss: 0.0013
Epoch 45/100
33/33 [==============================] - 2s 47ms/step - loss: 6.2130e-04 - val_loss: 0.0013
Epoch 46/100
33/33 [==============================] - 2s 47ms/step - loss: 6.0866e-04 - val_loss: 0.0013
Epoch 47/100
33/33 [==============================] - 2s 47ms/step - loss: 5.9750e-04 - val_loss: 0.0013
Epoch 48/100
33/33 [==============================] - 2s 52ms/step - loss: 5.8664e-04 - val_loss: 0.0012
Epoch 49/100
33/33 [==============================] - 2s 50ms/step - loss: 5.8380e-04 - val_loss: 0.0013
Epoch 50/100
33/33 [==============================] - 2s 47ms/step - loss: 5.7196e-04 - val_loss: 0.0012
Epoch 51/100
33/33 [==============================] - 2s 46ms/step - loss: 5.6158e-04 - val_loss: 0.0012
Epoch 52/100
33/33 [==============================] - 2s 48ms/step - loss: 5.5605e-04 - val_loss: 0.0012
Epoch 53/100
33/33 [==============================] - 2s 49ms/step - loss: 5.4776e-04 - val_loss: 0.0013
Epoch 54/100
33/33 [==============================] - 2s 46ms/step - loss: 5.5986e-04 - val_loss: 0.0011
Epoch 55/100
33/33 [==============================] - 2s 46ms/step - loss: 5.4152e-04 - val_loss: 0.0011
Epoch 56/100
33/33 [==============================] - 2s 48ms/step - loss: 5.2678e-04 - val_loss: 0.0012
Epoch 57/100
33/33 [==============================] - 2s 46ms/step - loss: 5.2482e-04 - val_loss: 0.0011
Epoch 58/100
33/33 [==============================] - 2s 48ms/step - loss: 5.1398e-04 - val_loss: 0.0011
Epoch 59/100
33/33 [==============================] - 2s 48ms/step - loss: 5.0483e-04 - val_loss: 0.0011
Epoch 60/100
33/33 [==============================] - 2s 46ms/step - loss: 5.1063e-04 - val_loss: 0.0011
Epoch 61/100
33/33 [==============================] - 2s 46ms/step - loss: 4.9780e-04 - val_loss: 0.0011
Epoch 62/100
33/33 [==============================] - 2s 48ms/step - loss: 4.8840e-04 - val_loss: 0.0011
Epoch 63/100
33/33 [==============================] - 2s 48ms/step - loss: 4.8629e-04 - val_loss: 0.0011
Epoch 64/100
33/33 [==============================] - 2s 47ms/step - loss: 4.8696e-04 - val_loss: 0.0011
Epoch 65/100
33/33 [==============================] - 2s 49ms/step - loss: 4.7261e-04 - val_loss: 0.0011
Epoch 66/100
33/33 [==============================] - 2s 48ms/step - loss: 4.7972e-04 - val_loss: 0.0011
Epoch 67/100
33/33 [==============================] - 2s 48ms/step - loss: 4.6733e-04 - val_loss: 0.0010
Epoch 68/100
33/33 [==============================] - 2s 52ms/step - loss: 4.6632e-04 - val_loss: 0.0010
Epoch 69/100
33/33 [==============================] - 2s 50ms/step - loss: 4.5778e-04 - val_loss: 0.0010
Epoch 70/100
33/33 [==============================] - 2s 48ms/step - loss: 4.5232e-04 - val_loss: 0.0011
Epoch 71/100
33/33 [==============================] - 2s 46ms/step - loss: 4.5273e-04 - val_loss: 0.0010
Epoch 72/100
33/33 [==============================] - 2s 47ms/step - loss: 4.4495e-04 - val_loss: 0.0010
Epoch 73/100
33/33 [==============================] - 2s 48ms/step - loss: 4.3983e-04 - val_loss: 0.0010
Epoch 74/100
33/33 [==============================] - 2s 46ms/step - loss: 4.3311e-04 - val_loss: 9.9221e-04
Epoch 75/100
33/33 [==============================] - 2s 48ms/step - loss: 4.3601e-04 - val_loss: 9.8385e-04
Epoch 76/100
33/33 [==============================] - 2s 48ms/step - loss: 4.3815e-04 - val_loss: 9.7775e-04
Epoch 77/100
33/33 [==============================] - 2s 49ms/step - loss: 4.2336e-04 - val_loss: 9.9015e-04
Epoch 78/100
33/33 [==============================] - 2s 47ms/step - loss: 4.2363e-04 - val_loss: 9.6977e-04
Epoch 79/100
33/33 [==============================] - 2s 46ms/step - loss: 4.2516e-04 - val_loss: 9.8541e-04
Epoch 80/100
33/33 [==============================] - 2s 48ms/step - loss: 4.1616e-04 - val_loss: 9.7897e-04
Epoch 81/100
33/33 [==============================] - 2s 48ms/step - loss: 4.1672e-04 - val_loss: 9.4984e-04
Epoch 82/100
33/33 [==============================] - 2s 46ms/step - loss: 4.0871e-04 - val_loss: 9.5117e-04
Epoch 83/100
33/33 [==============================] - 2s 48ms/step - loss: 4.2330e-04 - val_loss: 9.4208e-04
Epoch 84/100
33/33 [==============================] - 2s 48ms/step - loss: 4.0746e-04 - val_loss: 9.4401e-04
Epoch 85/100
33/33 [==============================] - 2s 48ms/step - loss: 4.0109e-04 - val_loss: 9.4433e-04
Epoch 86/100
33/33 [==============================] - 2s 48ms/step - loss: 4.0284e-04 - val_loss: 0.0010
Epoch 87/100
33/33 [==============================] - 2s 48ms/step - loss: 3.9930e-04 - val_loss: 9.2981e-04
Epoch 88/100
33/33 [==============================] - 2s 54ms/step - loss: 3.9107e-04 - val_loss: 9.2976e-04
Epoch 89/100
33/33 [==============================] - 2s 53ms/step - loss: 3.8941e-04 - val_loss: 9.4429e-04
Epoch 90/100
33/33 [==============================] - 2s 50ms/step - loss: 3.8679e-04 - val_loss: 9.1208e-04
Epoch 91/100
33/33 [==============================] - 2s 50ms/step - loss: 3.8599e-04 - val_loss: 9.0850e-04
Epoch 92/100
33/33 [==============================] - 2s 48ms/step - loss: 3.8494e-04 - val_loss: 9.1116e-04
Epoch 93/100
33/33 [==============================] - 2s 47ms/step - loss: 3.7639e-04 - val_loss: 9.3391e-04
Epoch 94/100
33/33 [==============================] - 2s 47ms/step - loss: 3.7833e-04 - val_loss: 9.0607e-04
Epoch 95/100
33/33 [==============================] - 2s 48ms/step - loss: 3.7497e-04 - val_loss: 8.9301e-04
Epoch 96/100
33/33 [==============================] - 2s 47ms/step - loss: 3.7287e-04 - val_loss: 8.8362e-04
Epoch 97/100
33/33 [==============================] - 2s 49ms/step - loss: 3.7071e-04 - val_loss: 8.7167e-04
Epoch 98/100
33/33 [==============================] - 2s 47ms/step - loss: 3.7310e-04 - val_loss: 8.6366e-04
Epoch 99/100
33/33 [==============================] - 2s 48ms/step - loss: 3.7395e-04 - val_loss: 8.7302e-04
Epoch 100/100
33/33 [==============================] - 2s 48ms/step - loss: 3.6897e-04 - val_loss: 9.2363e-04
#Plotting the loss
plt.plot(model1.history['loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

train_Predict=model.predict(X_train)              
test_Predict=model.predict(X_test)
train_Predict=scaler1.inverse_transform(train_Predict)       #getting back original form for plotting
test_Predict=scaler2.inverse_transform(test_Predict)
train_data1=scaler1.inverse_transform(Y_train)
test_data1=scaler2.inverse_transform(Y_test)
66/66 [==============================] - 1s 11ms/step
15/15 [==============================] - 0s 11ms/step
# Getting rmse of train data
print("LSTM Model Train RMSE %0.2f" % (math.sqrt(mean_squared_error(train_data1,train_Predict))))
LSTM Model Train RMSE 0.63
# Getting rmse of test data
print("LSTM Model Test RMSE %0.2f" %(math.sqrt(mean_squared_error(test_data1,test_Predict))))
LSTM Model Test RMSE 0.90
df_1 = pd.DataFrame(test_data1, columns = ['Original_Test_Data'])
df_2 = pd.DataFrame(test_Predict, columns = ['Predicted_Test_Data'])
df_1.reset_index()
df_2.reset_index()
result = pd.merge(df_1, df_2, left_index=True, right_index=True)
result.head(10)
Original_Test_Data	Predicted_Test_Data
0	21.170	21.414682
1	21.354	21.460148
2	21.500	21.516184
3	21.007	21.587000
4	21.346	21.573744
5	21.063	21.589682
6	20.907	21.562263
7	21.044	21.500355
8	21.227	21.455038
9	21.277	21.445774
#plotting the models performance against the  data
plt.figure(figsize = (18,6))

plt.subplot(1,2,1)
plt.title('Prediction in terms of Training Data',fontsize=10)
plt.plot(train_data1,label='Actual')
plt.plot(train_Predict,label='Predicted')
leg=plt.legend()

plt.subplot(1,2,2)
plt.title('Prediction in terms of Test data',fontsize=10)
plt.plot(test_data1,label='Actual')
plt.plot(test_Predict,label='Predicted')
leg=plt.legend()
plt.show()

#Creating the Ensemble model 

# Initialising the  Model
model=Sequential()   
model.add(SimpleRNN(7,input_shape=(timestep,no_of_features),return_sequences=True))
# Dropout layer to avoid overfitting
# model.add(Dropout(0.3))
# LSTM(hidden neurons, Carries output to next hidden layer )
model.add(LSTM(7,return_sequences=False,input_shape=(timestep,no_of_features)))    
# dense layer for output
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
# model summary
model.summary()  
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_1 (SimpleRNN)    (None, 100, 7)            63        
                                                                 
 lstm_1 (LSTM)               (None, 7)                 420       
                                                                 
 dense_2 (Dense)             (None, 1)                 8         
                                                                 
=================================================================
Total params: 491
Trainable params: 491
Non-trainable params: 0
_________________________________________________________________
# fitting the model on training dataset
model1=model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=100,batch_size=64,verbose=1) 
Epoch 1/100
33/33 [==============================] - 7s 103ms/step - loss: 0.0275 - val_loss: 0.0156
Epoch 2/100
33/33 [==============================] - 2s 73ms/step - loss: 0.0059 - val_loss: 0.0045
Epoch 3/100
33/33 [==============================] - 2s 72ms/step - loss: 0.0021 - val_loss: 0.0027
Epoch 4/100
33/33 [==============================] - 2s 73ms/step - loss: 0.0013 - val_loss: 0.0021
Epoch 5/100
33/33 [==============================] - 2s 74ms/step - loss: 0.0011 - val_loss: 0.0018
Epoch 6/100
33/33 [==============================] - 2s 72ms/step - loss: 8.9802e-04 - val_loss: 0.0017
Epoch 7/100
33/33 [==============================] - 2s 73ms/step - loss: 8.0436e-04 - val_loss: 0.0015
Epoch 8/100
33/33 [==============================] - 2s 72ms/step - loss: 7.3303e-04 - val_loss: 0.0014
Epoch 9/100
33/33 [==============================] - 2s 74ms/step - loss: 6.8548e-04 - val_loss: 0.0014
Epoch 10/100
33/33 [==============================] - 2s 75ms/step - loss: 6.5814e-04 - val_loss: 0.0014
Epoch 11/100
33/33 [==============================] - 2s 73ms/step - loss: 6.2842e-04 - val_loss: 0.0013
Epoch 12/100
33/33 [==============================] - 2s 71ms/step - loss: 6.1638e-04 - val_loss: 0.0013
Epoch 13/100
33/33 [==============================] - 2s 72ms/step - loss: 5.8722e-04 - val_loss: 0.0013
Epoch 14/100
33/33 [==============================] - 2s 73ms/step - loss: 5.6408e-04 - val_loss: 0.0014
Epoch 15/100
33/33 [==============================] - 2s 73ms/step - loss: 5.7377e-04 - val_loss: 0.0012
Epoch 16/100
33/33 [==============================] - 2s 73ms/step - loss: 5.4756e-04 - val_loss: 0.0012
Epoch 17/100
33/33 [==============================] - 2s 71ms/step - loss: 5.3850e-04 - val_loss: 0.0013
Epoch 18/100
33/33 [==============================] - 2s 73ms/step - loss: 5.3399e-04 - val_loss: 0.0011
Epoch 19/100
33/33 [==============================] - 2s 71ms/step - loss: 5.2253e-04 - val_loss: 0.0012
Epoch 20/100
33/33 [==============================] - 2s 71ms/step - loss: 5.1232e-04 - val_loss: 0.0012
Epoch 21/100
33/33 [==============================] - 2s 72ms/step - loss: 5.1544e-04 - val_loss: 0.0012
Epoch 22/100
33/33 [==============================] - 2s 73ms/step - loss: 5.0685e-04 - val_loss: 0.0011
Epoch 23/100
33/33 [==============================] - 2s 73ms/step - loss: 5.0400e-04 - val_loss: 0.0011
Epoch 24/100
33/33 [==============================] - 3s 78ms/step - loss: 4.9224e-04 - val_loss: 0.0011
Epoch 25/100
33/33 [==============================] - 2s 71ms/step - loss: 4.8480e-04 - val_loss: 0.0011
Epoch 26/100
33/33 [==============================] - 2s 72ms/step - loss: 4.9715e-04 - val_loss: 0.0011
Epoch 27/100
33/33 [==============================] - 2s 72ms/step - loss: 5.0695e-04 - val_loss: 0.0011
Epoch 28/100
33/33 [==============================] - 2s 74ms/step - loss: 4.8042e-04 - val_loss: 0.0011
Epoch 29/100
33/33 [==============================] - 2s 72ms/step - loss: 4.7238e-04 - val_loss: 0.0011
Epoch 30/100
33/33 [==============================] - 2s 73ms/step - loss: 4.9157e-04 - val_loss: 0.0010
Epoch 31/100
33/33 [==============================] - 2s 71ms/step - loss: 4.7869e-04 - val_loss: 0.0010
Epoch 32/100
33/33 [==============================] - 2s 72ms/step - loss: 4.8780e-04 - val_loss: 0.0010
Epoch 33/100
33/33 [==============================] - 2s 72ms/step - loss: 4.7987e-04 - val_loss: 0.0010
Epoch 34/100
33/33 [==============================] - 2s 72ms/step - loss: 4.8112e-04 - val_loss: 0.0010
Epoch 35/100
33/33 [==============================] - 3s 76ms/step - loss: 4.7134e-04 - val_loss: 0.0011
Epoch 36/100
33/33 [==============================] - 2s 72ms/step - loss: 4.6021e-04 - val_loss: 0.0011
Epoch 37/100
33/33 [==============================] - 3s 79ms/step - loss: 4.4563e-04 - val_loss: 9.8557e-04
Epoch 38/100
33/33 [==============================] - 2s 71ms/step - loss: 4.5692e-04 - val_loss: 9.7956e-04
Epoch 39/100
33/33 [==============================] - 2s 73ms/step - loss: 4.4089e-04 - val_loss: 9.9138e-04
Epoch 40/100
33/33 [==============================] - 2s 72ms/step - loss: 4.3596e-04 - val_loss: 9.8291e-04
Epoch 41/100
33/33 [==============================] - 2s 74ms/step - loss: 4.6449e-04 - val_loss: 0.0010
Epoch 42/100
33/33 [==============================] - 2s 74ms/step - loss: 4.4062e-04 - val_loss: 9.8173e-04
Epoch 43/100
33/33 [==============================] - 2s 74ms/step - loss: 4.2463e-04 - val_loss: 9.5026e-04
Epoch 44/100
33/33 [==============================] - 2s 72ms/step - loss: 4.4977e-04 - val_loss: 9.8994e-04
Epoch 45/100
33/33 [==============================] - 2s 71ms/step - loss: 4.2490e-04 - val_loss: 9.3710e-04
Epoch 46/100
33/33 [==============================] - 2s 73ms/step - loss: 4.1914e-04 - val_loss: 9.4971e-04
Epoch 47/100
33/33 [==============================] - 2s 71ms/step - loss: 4.1321e-04 - val_loss: 9.4873e-04
Epoch 48/100
33/33 [==============================] - 2s 71ms/step - loss: 4.1351e-04 - val_loss: 9.4984e-04
Epoch 49/100
33/33 [==============================] - 2s 72ms/step - loss: 4.0561e-04 - val_loss: 0.0010
Epoch 50/100
33/33 [==============================] - 3s 78ms/step - loss: 4.2467e-04 - val_loss: 0.0010
Epoch 51/100
33/33 [==============================] - 2s 72ms/step - loss: 4.0665e-04 - val_loss: 9.3772e-04
Epoch 52/100
33/33 [==============================] - 2s 73ms/step - loss: 3.9521e-04 - val_loss: 9.1106e-04
Epoch 53/100
33/33 [==============================] - 2s 71ms/step - loss: 4.0632e-04 - val_loss: 8.9336e-04
Epoch 54/100
33/33 [==============================] - 2s 72ms/step - loss: 4.1510e-04 - val_loss: 8.8757e-04
Epoch 55/100
33/33 [==============================] - 2s 73ms/step - loss: 4.2078e-04 - val_loss: 8.9575e-04
Epoch 56/100
33/33 [==============================] - 2s 71ms/step - loss: 4.0632e-04 - val_loss: 8.7530e-04
Epoch 57/100
33/33 [==============================] - 2s 72ms/step - loss: 3.9821e-04 - val_loss: 8.7465e-04
Epoch 58/100
33/33 [==============================] - 2s 72ms/step - loss: 4.0490e-04 - val_loss: 8.9217e-04
Epoch 59/100
33/33 [==============================] - 2s 72ms/step - loss: 3.9211e-04 - val_loss: 8.7070e-04
Epoch 60/100
33/33 [==============================] - 2s 73ms/step - loss: 4.2071e-04 - val_loss: 8.8157e-04
Epoch 61/100
33/33 [==============================] - 2s 71ms/step - loss: 3.9017e-04 - val_loss: 8.5876e-04
Epoch 62/100
33/33 [==============================] - 2s 72ms/step - loss: 3.9456e-04 - val_loss: 8.6452e-04
Epoch 63/100
33/33 [==============================] - 2s 74ms/step - loss: 3.7014e-04 - val_loss: 8.5884e-04
Epoch 64/100
33/33 [==============================] - 2s 75ms/step - loss: 3.7495e-04 - val_loss: 8.5203e-04
Epoch 65/100
33/33 [==============================] - 2s 72ms/step - loss: 3.8159e-04 - val_loss: 8.4424e-04
Epoch 66/100
33/33 [==============================] - 2s 72ms/step - loss: 3.7007e-04 - val_loss: 8.5925e-04
Epoch 67/100
33/33 [==============================] - 2s 73ms/step - loss: 3.7242e-04 - val_loss: 8.6129e-04
Epoch 68/100
33/33 [==============================] - 2s 74ms/step - loss: 3.7056e-04 - val_loss: 8.3820e-04
Epoch 69/100
33/33 [==============================] - 3s 78ms/step - loss: 3.6124e-04 - val_loss: 8.3815e-04
Epoch 70/100
33/33 [==============================] - 2s 72ms/step - loss: 3.6051e-04 - val_loss: 9.2345e-04
Epoch 71/100
33/33 [==============================] - 2s 73ms/step - loss: 3.8053e-04 - val_loss: 8.9421e-04
Epoch 72/100
33/33 [==============================] - 2s 73ms/step - loss: 3.8022e-04 - val_loss: 8.1287e-04
Epoch 73/100
33/33 [==============================] - 2s 73ms/step - loss: 3.6640e-04 - val_loss: 8.0785e-04
Epoch 74/100
33/33 [==============================] - 2s 72ms/step - loss: 3.7952e-04 - val_loss: 9.0956e-04
Epoch 75/100
33/33 [==============================] - 2s 71ms/step - loss: 3.9036e-04 - val_loss: 0.0010
Epoch 76/100
33/33 [==============================] - 2s 72ms/step - loss: 3.8491e-04 - val_loss: 8.6445e-04
Epoch 77/100
33/33 [==============================] - 3s 78ms/step - loss: 3.6336e-04 - val_loss: 8.0260e-04
Epoch 78/100
33/33 [==============================] - 2s 74ms/step - loss: 3.4940e-04 - val_loss: 7.9458e-04
Epoch 79/100
33/33 [==============================] - 2s 72ms/step - loss: 3.5688e-04 - val_loss: 8.0178e-04
Epoch 80/100
33/33 [==============================] - 2s 72ms/step - loss: 3.5355e-04 - val_loss: 8.4700e-04
Epoch 81/100
33/33 [==============================] - 2s 72ms/step - loss: 3.4007e-04 - val_loss: 8.6211e-04
Epoch 82/100
33/33 [==============================] - 2s 74ms/step - loss: 3.7155e-04 - val_loss: 8.3154e-04
Epoch 83/100
33/33 [==============================] - 3s 76ms/step - loss: 3.4291e-04 - val_loss: 8.0641e-04
Epoch 84/100
33/33 [==============================] - 2s 73ms/step - loss: 3.4405e-04 - val_loss: 8.0770e-04
Epoch 85/100
33/33 [==============================] - 2s 73ms/step - loss: 3.3517e-04 - val_loss: 7.8994e-04
Epoch 86/100
33/33 [==============================] - 2s 72ms/step - loss: 3.3020e-04 - val_loss: 7.7102e-04
Epoch 87/100
33/33 [==============================] - 2s 74ms/step - loss: 3.3365e-04 - val_loss: 7.7648e-04
Epoch 88/100
33/33 [==============================] - 2s 73ms/step - loss: 3.3104e-04 - val_loss: 7.5462e-04
Epoch 89/100
33/33 [==============================] - 2s 73ms/step - loss: 3.2795e-04 - val_loss: 7.7639e-04
Epoch 90/100
33/33 [==============================] - 3s 79ms/step - loss: 3.3515e-04 - val_loss: 8.3007e-04
Epoch 91/100
33/33 [==============================] - 2s 74ms/step - loss: 3.3004e-04 - val_loss: 7.5253e-04
Epoch 92/100
33/33 [==============================] - 2s 74ms/step - loss: 3.4952e-04 - val_loss: 7.4648e-04
Epoch 93/100
33/33 [==============================] - 2s 73ms/step - loss: 3.3517e-04 - val_loss: 8.6737e-04
Epoch 94/100
33/33 [==============================] - 2s 73ms/step - loss: 3.4223e-04 - val_loss: 7.5060e-04
Epoch 95/100
33/33 [==============================] - 2s 72ms/step - loss: 3.2291e-04 - val_loss: 7.3749e-04
Epoch 96/100
33/33 [==============================] - 2s 75ms/step - loss: 3.3399e-04 - val_loss: 7.8126e-04
Epoch 97/100
33/33 [==============================] - 2s 72ms/step - loss: 3.1731e-04 - val_loss: 7.3206e-04
Epoch 98/100
33/33 [==============================] - 2s 71ms/step - loss: 3.1431e-04 - val_loss: 7.6063e-04
Epoch 99/100
33/33 [==============================] - 2s 71ms/step - loss: 3.1232e-04 - val_loss: 7.3476e-04
Epoch 100/100
33/33 [==============================] - 2s 72ms/step - loss: 3.1552e-04 - val_loss: 7.5787e-04
#Plotting the loss
plt.plot(model1.history['loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

train_Predict=model.predict(X_train)              
test_Predict=model.predict(X_test)
train_Predict=scaler1.inverse_transform(train_Predict)       #getting back original form for plotting
test_Predict=scaler2.inverse_transform(test_Predict)
train_data1=scaler1.inverse_transform(Y_train)
test_data1=scaler2.inverse_transform(Y_test)
66/66 [==============================] - 2s 18ms/step
15/15 [==============================] - 0s 17ms/step
# Getting rmse of train data
print("RNN+LSTM Train RMSE %0.2f"%(math.sqrt(mean_squared_error(train_data1,train_Predict))))
RNN+LSTM Train RMSE 0.58
# Getting rmse of test data
print("RNN+LSTM Test RMSE %0.2f"%(math.sqrt(mean_squared_error(test_data1,test_Predict))))
RNN+LSTM Test RMSE 0.81
df_1 = pd.DataFrame(test_data1, columns = ['Original_Test_Data'])
df_2 = pd.DataFrame(test_Predict, columns = ['Predicted_Test_Data'])
df_1.reset_index()
df_2.reset_index()
result = pd.merge(df_1, df_2, left_index=True, right_index=True)
result.head(10)
Original_Test_Data	Predicted_Test_Data
0	21.170	21.571133
1	21.354	21.458334
2	21.500	21.376024
3	21.007	21.450132
4	21.346	21.332624
5	21.063	21.320848
6	20.907	21.256767
7	21.044	21.122128
8	21.227	21.083820
9	21.277	21.110106
#plotting the models performance against the  data
plt.figure(figsize = (18,6))

plt.subplot(1,2,1)
plt.title('Prediction in terms of Training Data',fontsize=10)
plt.plot(train_data1,label='Actual')
plt.plot(train_Predict,label='Predicted')
leg=plt.legend()

plt.subplot(1,2,2)
plt.title('Prediction in terms of Test data',fontsize=10)
plt.plot(test_data1,label='Actual')
plt.plot(test_Predict,label='Predicted')
leg=plt.legend()
plt.show()
